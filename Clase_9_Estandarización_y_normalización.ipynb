{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase 9: Estandarización y normalización.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MTracchia/Laboratorio-de-Datos/blob/main/Clase_9_Estandarizaci%C3%B3n_y_normalizaci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wNE-MMUtMdT"
      },
      "source": [
        "# Atacando un problema de clasificación: predicción de días lluviosos (segunda parte)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsovLOVq3ecd"
      },
      "source": [
        "# ¿Qué pasa si los datos no están normalizados?\n",
        "\n",
        "Empezamos igual que en el notebook anterior, cargando los datos de INTA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYHZEEWqtJzm"
      },
      "source": [
        "# Como siempre, tratamos de traer todos los paquetes al ppio\n",
        "from google.colab import drive # Para montar nuestro drive en la consola\n",
        "import matplotlib.pylab as plt # Para gráficos\n",
        "import numpy as np # Para manejo de arrays, operaciones matemáticas, etc.\n",
        "from sklearn.linear_model import LogisticRegression # El método de regresión logística que vamos a usar\n",
        "import pandas as pd # Para manejo de base de datos\n",
        "from sklearn.metrics import confusion_matrix # matriz de confusion\n",
        "\n",
        "# Traemos los datos\n",
        "drive.mount('/content/drive') # Montamos nuestra unidad de Google Drive\n",
        "\n",
        "filename = '/content/drive/My Drive/LaboDatos2021/datosDiariosSanFernandoINTA.xls'\n",
        "\n",
        "d = pd.read_excel(filename) # Levantamos los datos, en este caso, con el método pd.read_excel\n",
        "d.head() # Mostramos las primeras líneas, para darnos una idea"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1PLrdfuz88A"
      },
      "source": [
        "Luego, selecciono un subconjunto de columnas, las renombro y me deshago de los datos faltantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIqZOezjtclG"
      },
      "source": [
        "d_filtrado = d[['Temperatura_Abrigo_150cm_Maxima',\n",
        "                'Temperatura_Abrigo_150cm_Minima',\n",
        "                'Precipitacion_Pluviometrica',\n",
        "                'Velocidad_Viento_Maxima',\n",
        "                'Rocio_Medio',\n",
        "                'Humedad_Media']].dropna().copy() # Nos quedamos con ciertos campos del data set, para facilitar el trabajo. Y para limitarlo.\n",
        "                                                  # Notar que primero aplicamos el método .dropna() para eliminar filas que tengan alguna columna con NaN \n",
        "                                                  # Ademas, el metodo copy() nos asegura que estemos creando un nuevo dataframe \n",
        "d_filtrado.rename({'Temperatura_Abrigo_150cm_Maxima' : 'temperaturaMaxima',\n",
        "                   'Temperatura_Abrigo_150cm_Minima' : 'temperaturaMinima',\n",
        "                   'Precipitacion_Pluviometrica' : 'precipitacion',\n",
        "                   'Humedad_Media' : 'humedad',\n",
        "                   'Rocio_Medio' : 'rocio',\n",
        "                   'Velocidad_Viento_Maxima' : 'viento'},\n",
        "                  axis = 1,\n",
        "                  inplace = True) # Esto toma como input un diccionario en el cual las llaves son los nombres actuales de columnas, y los valores los nombres nuevos (a los que queremos renombrar)\n",
        "                                  # axis = 1 es porque queremos renombrar columnas, y inplace=True es porque queremos \"pisar\" el dataframe al renombrarlo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZU7c5X50CWx"
      },
      "source": [
        "Creo una nueva columna que contiene la etiqueta que indica si llueve o no llueve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUgztnCStg7n"
      },
      "source": [
        "d_filtrado['llueveNollueve'] = 0 # empezamos con una columna llena de 0. \n",
        "indice =  d_filtrado['precipitacion'] > 0  # esto me da los valores del indice para los cuales hay precipitacion mayor a 0\n",
        "d_filtrado.loc[indice, 'llueveNollueve'] = 1 # entonces para esos valores del indice pongo 1, porque en el dia correspondiente, llovio\n",
        "\n",
        "d_filtrado['llueveNollueve'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8FJMkGa0LN2"
      },
      "source": [
        "Aqui vuelvo a definir las funciones que defini en el notebook anterior, que sirven para graficar y para calcular la balanced accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyAeA1bEuj3d"
      },
      "source": [
        "def grafica_logred_2D(X,y,regLog):\n",
        "\n",
        "  beta_0 = regLog.intercept_ # El beta 0\n",
        "  beta_1 = regLog.coef_[0][0] # El coeficiente beta_1\n",
        "  beta_2 = regLog.coef_[0][1] # El coeficiente beta_2\n",
        "\n",
        "  fig, ax = plt.subplots(figsize = (10,7))\n",
        "\n",
        "  ax.set_title('Mi Regresión Logística')\n",
        "  ax.scatter(X[:,0], # Ploteamos la primera columna de nuestra matriz X. Osea, la temperaturaMinima\n",
        "            X[:,1], # Ploteamos la segunda columna de nuestra matriz X. Osea, la humedad\n",
        "            c = y, # Les damos color a los puntos según la etiqueta real\n",
        "            cmap = 'Set3', # El mapa de color\n",
        "            )\n",
        "  ax.set_xlabel('Temperatura Mínima') # Etiqueta del eje x\n",
        "  ax.set_ylabel('Humedad') # Etiqueta del eje y\n",
        "\n",
        "  # Plotear la curva acá, no sería de mucho interés. Sí podríamos colorear un área como representación de la función P: R² ---> R\n",
        "  # Plotiemos la frontera de decisión\n",
        "\n",
        "  x_1_test = np.arange(-20,30,0.5) # Generamos unas temperaturas de prueba, para plotear la definición de frontera en el caso de dos features como vimos en la teórica\n",
        "  ax.plot(x_1_test,\n",
        "          - (beta_0 + x_1_test * beta_1) / beta_2, # Generamos el la recta que calculamos en la teórica\n",
        "          linestyle = 'dashed',\n",
        "          color = 'k')\n",
        "  #ax.legend()\n",
        "  ax.axis([min(X[:,0]),max(X[:,0]),min(X[:,1]),max(X[:,1])]) # Seteamos los límites de los ejes\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def balanced_accuracy(cm): # funcion para calcular la \"balanced accuracy\"\n",
        "  sensibilidad = cm[1,1]/(cm[1,1]+cm[0,1])\n",
        "  especificidad = cm[0,0]/(cm[1,0]+cm[0,0])\n",
        "  return [sensibilidad, especificidad, (sensibilidad + especificidad)/2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib4kPzeY0TZg"
      },
      "source": [
        "Ahora vamos a usar las variables de temperatura maxima y humedad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfr7gbDqtjIm"
      },
      "source": [
        "# Definamos la matriz X\n",
        "campos = ['temperaturaMinima',\n",
        "          'humedad'] # Lista que contiene las features de interés. En este caso, son temperatura mínima y humedad\n",
        "X = d_filtrado[campos].values # En este caso no hace falta reshapear, porque ya tiene la forma que queremos\n",
        "print(X.shape) # Como vemos, es una matriz de 1054 filas y 2 columnas\n",
        "y = d_filtrado['llueveNollueve'] # Nuestra etiqueta sigue siende la misma de antes\n",
        "\n",
        "regLog = LogisticRegression(penalty = 'none', class_weight='balanced') # Inicializamos nuevamente el modelo\n",
        "regLog.fit(X, y) # Ajustamos el modelo con los parámetros\n",
        "score = regLog.score(X,y) # Calculamos el score\n",
        "beta_0 = regLog.intercept_ # El beta 0\n",
        "beta_1 = regLog.coef_[0][0] # El coeficiente beta_1\n",
        "beta_2 = regLog.coef_[0][1] # El coeficiente beta_2\n",
        "\n",
        "ypred = regLog.predict(X) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "cm = confusion_matrix(ypred, y) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "metricas = balanced_accuracy(cm)\n",
        "\n",
        "print('El score del modelo es de: {}'.format(round(score,4))) # Le pido que printee el score del modelo. Le pido que lo haga con 4 cifras significativas\n",
        "print('Matriz de confusion del modelo es:')\n",
        "print(cm)\n",
        "print('Sensibilidad del modelo es de: {}'.format(round(metricas[0],4)))  \n",
        "print('Especificidad del modelo es de: {}'.format(round(metricas[1],4)))  \n",
        "print('BA del modelo es de: {}'.format(round(metricas[2],4))) \n",
        "\n",
        "grafica_logred_2D(X,y,regLog)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNTkuMXA0a_P"
      },
      "source": [
        "Podemos graficar histogramas para verificar que el rango de valores que toman estas variables es muy similar, por lo tanto no tengo un problema de escalas diferentes en mis variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDluC80Lt0Gv"
      },
      "source": [
        "plt.hist(X[:,0], bins='auto')  # construimos histograma para la primera variable, con bines seleccionados automáticamente\n",
        "plt.title(\"Histograma de temperatura minima\") # ponemos titulo\n",
        "plt.show() # mostrar!\n",
        "\n",
        "plt.hist(X[:,1], bins='auto')  # construimos histograma para la segunda variable, con bines seleccionados automáticamente\n",
        "plt.title(\"Histograma de humedad\") # ponemos titulo\n",
        "plt.show() # mostrar!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAazDvkC0kl9"
      },
      "source": [
        "Ahora, podemos suponer que los datos de humedad estaban en una unidad diferente, por ejemplo, una unidad que resulta en multiplicar por 1000, y ahi serian bien distintos los rangos de valores de las columnas.\n",
        "Observamos el ejemplo de multiplicar por 1000 la columna de la humedad:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZySm41-hvRJD"
      },
      "source": [
        "# Definamos la matriz X\n",
        "campos = ['temperaturaMinima',\n",
        "          'humedad'] # Lista que contiene las features de interés\n",
        "X = d_filtrado[campos].values # En este caso no hace falta reshapear, porque ya tiene la forma que queremos\n",
        "\n",
        "X[:,1] = 1000*X[:,1] # supongamos que cambiamos las unidades con las que medimos la humedad\n",
        "\n",
        "regLog = LogisticRegression(penalty = 'none', class_weight='balanced') # Inicializamos nuevamente el modelo\n",
        "regLog.fit(X, y) # Ajustamos el modelo con los parámetros\n",
        "score = regLog.score(X,y) # Calculamos el score\n",
        "beta_0 = regLog.intercept_ # El beta 0\n",
        "beta_1 = regLog.coef_[0][0] # El coeficiente beta_1\n",
        "beta_2 = regLog.coef_[0][1] # El coeficiente beta_2\n",
        "\n",
        "ypred = regLog.predict(X) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "cm = confusion_matrix(ypred, y) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "metricas = balanced_accuracy(cm)\n",
        "\n",
        "print('El score del modelo es de: {}'.format(round(score,4))) # Le pido que printee el score del modelo. Le pido que lo haga con 4 cifras significativas\n",
        "print('Matriz de confusion del modelo es:')\n",
        "print(cm)\n",
        "print('Sensibilidad del modelo es de: {}'.format(round(metricas[0],4)))  \n",
        "print('Especificidad del modelo es de: {}'.format(round(metricas[1],4)))  \n",
        "print('BA del modelo es de: {}'.format(round(metricas[2],4))) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjGgSObD02OS"
      },
      "source": [
        "Vemos que la performance del modelo cae drasticamente. Esto es claramente un problema: si me hubiesen dado los datos con la humedad medida en esta unidad, hubiese tenido problemas para alcanzar una performance mínimamente aceptable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rF40mco3y6x"
      },
      "source": [
        "# ¿Cómo puedo asegurarme de trabajar con datos normalizados?\n",
        "\n",
        "Por suerte, scikit-learn viene con funciones que sirven para normalizar y estandarizar datos. Son las siguientes:\n",
        "\n",
        "```\n",
        "sklearn.preprocessing.MinMaxScaler\n",
        "\n",
        "sklearn.preprocessing.StandardScaler\n",
        "```\n",
        "\n",
        "La primera implementa el escaleo MinMax, escaleando todas la variables en el intervalo [0,1]. La segunda transforma las variables a sus z-scores.\n",
        "\n",
        "Vamos a normalizar la matriz de features X, usando la primera de estas dos funciones.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOgu2iRN4qSG"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler() # primero creo un objeto MinMaxScaler. Por defecto, esto normaliza los datos al intervalo [0,1]\n",
        "scaler.fit(X) # encuentro los parametros para el escaleo\n",
        "X_scale = scaler.transform(X) # aplico la transformacion\n",
        "\n",
        "plt.hist(X[:,1], bins='auto')  \n",
        "plt.title(\"Humedad\")\n",
        "plt.show()\n",
        "\n",
        "plt.hist(X_scale[:,1], bins='auto')  \n",
        "plt.title(\"Humedad (escaleada)\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPV6i5WA50ec"
      },
      "source": [
        "Finalmente, observo que recupero la performance si ahora entreno la regresión logística con los datos re-escaleados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pGAfIfA6FNI"
      },
      "source": [
        "regLog = LogisticRegression(penalty = 'none', class_weight='balanced') # Inicializamos nuevamente el modelo\n",
        "regLog.fit(X_scale, y) # Ajustamos el modelo con los parámetros\n",
        "score = regLog.score(X_scale,y) # Calculamos el score\n",
        "\n",
        "ypred = regLog.predict(X_scale) # con esto obtengo la predicción de las etiquetas en base a mis datos\n",
        "cm = confusion_matrix(ypred, y) # primera entrada son las etiquetas predichas, segunda son las reales\n",
        "metricas = balanced_accuracy(cm)\n",
        "\n",
        "print('El score del modelo es de: {}'.format(round(score,4))) # Le pido que printee el score del modelo. Le pido que lo haga con 4 cifras significativas\n",
        "print('Matriz de confusion del modelo es:')\n",
        "print(cm)\n",
        "print('Sensibilidad del modelo es de: {}'.format(round(metricas[0],4)))  \n",
        "print('Especificidad del modelo es de: {}'.format(round(metricas[1],4)))  \n",
        "print('BA del modelo es de: {}'.format(round(metricas[2],4))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dtltp816VpL"
      },
      "source": [
        "#  Para llevarse de este notebook\n",
        "\n",
        "*   Es buena práctica visualizar mis variables numéricas antes de entrenar un clasificador. Por ejemplo, podemos hacer histogramas. De esta forma, si una variable tiene un rango mucho más distinto que las demás, podemos identificarla.\n",
        "\n",
        "*   La performance del algoritmo puede estar influenciada por estas diferencias de escala. Esto no sucede para todos los algoritmos de machine learning, pero sí sucede para muchos. Por eso, siempre es buena idea estandarizar o normalizar los datos.\n",
        "\n",
        "*   El proceso de normalizar o estandarizar los datos es similar a lo que venimos viendo en scikit-learn. Primero creo un objeto, luego estimo sus parámetros, y finalmente los aplico para transformar los datos.\n",
        "\n",
        "```\n",
        "scaler = MinMaxScaler() \n",
        "scaler.fit(X) \n",
        "X_scale = scaler.transform(X) \n",
        "```\n",
        "\n",
        "... y lo mismo para la estandarización.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IarFyUKB7XwG"
      },
      "source": [
        "# Preguntas extra\n",
        "\n",
        "\n",
        "*   **¿Qué pasa si yo entreno el MinMaxScaler con datos X, y luego uso el método .transform en otros datos distintos Y? ¿Puedo asegurar que las variables en la matriz Y también van a estar necesariamente entre [0,1]?**\n",
        "\n",
        "*   ¿En qué situación puede convenir usar más la conversión a z-scores en vez del escaleo MinMax?\n",
        "\n",
        "*   Ir cambiando el factor multiplicativo 1000 para tener una idea de en qué momento la diferencia entre el rango de las variables empieza a afectar la performance del clasificador.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}