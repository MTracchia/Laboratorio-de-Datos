# -*- coding: utf-8 -*-
"""Ejercitación 3, Marcos Tracchia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14vgva9JM-DBnJllUTytNhBlwotIkdHeE

Cargo los datos de la forma que muestra el enunciado de la consigna.
"""

from keras.datasets import mnist,fashion_mnist

(X_train_raw, y_train), (X_test_raw, y_test) = fashion_mnist.load_data()  # cargo los dataset de entrenamiento y testeo

# reshapeamos para obtener un vector de 784 elementos (features) por cada imagen (samples)
X_train = X_train_raw.reshape(60000, -1) # no tocamos el numero de samples, 60000, pero reshapeamos las demas dimensiones
X_test = X_test_raw.reshape(10000, -1) # no tocamos el numero de samples, 60000, pero reshapeamos las demas dimensiones

X_train = X_train.astype('float32') # transformamos el tipo de datos a "float32"
X_test = X_test.astype('float32') # transformamos el tipo de datos a "float32"

# normalizamos por el maximo valor que pueden tener los pixels para que los valores queden entre 0 y 1
X_train = X_train/255.0
X_test = X_test/255.0

# print the final input shape ready for training
print("Matriz de entrenamiento shape", X_train.shape)
print("Matriz de testeo shape", X_test.shape)

"""**Primer paso:** Hacer PCA sobre la matrix de datos de entrenamiento."""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

std_scale = StandardScaler()
std_scale.fit(X_train)
X_train_scaled = std_scale.transform(X_train)
X_test_scaled = std_scale.transform(X_test)
pca = PCA(n_components=None)
pca.fit(X_train_scaled)
X_pca = pca.transform(X_train_scaled)

print('Dimensiones de la matriz en componentes principales: {}'.format(X_pca.shape))
print(X_pca)

"""**Segundo paso:** Obtener y plotear la varianza explicada por cada componente y la varianza explicada acumulada vs. número de componentes."""

import matplotlib.pyplot as plt
import numpy as np

evr = pca.explained_variance_ratio_
fig, ax = plt.subplots(1, 2, figsize = (12, 4))

ax[0].plot(range(1, len(evr) + 1), evr, '.-', markersize = 10)
ax[0].set_ylabel('Fracción de varianza explicada')
ax[0].set_xlabel('Número de componente principal')
ax[0].grid()

varianza_acumulada = np.cumsum(evr)

ax[1].plot(range(1, len(evr) + 1), varianza_acumulada, '.-', markersize = 10)
ax[1].set_ylabel('Fracción acumulada de varianza explicada')
ax[1].set_xlabel('Cantidad de componentes principales')
ax[1].grid()

"""**Tercer paso:** Visualizar las primeros cinco componentes en orden de varianza explicada"""

for i in range(5):
  plt.imshow(pca.components_[i].reshape(28,28), interpolation='none', cmap="gray")
  plt.show()

"""**Cuarto paso:** Elegir algunas de las imágenes al azar y mostrar la reconstrucción obtenida usando 2, 10, 25, 50 y 100 componentes.

---


"""

fig, ax = plt.subplots(1, 5, figsize = (20, 10))
fig_index = 0 

for p in [2, 10, 25, 50, 100]:

  pca = PCA(n_components = p)

  pca.fit(X_train_scaled)
  X_pca = pca.transform(X_train_scaled)

  X_r = pca.inverse_transform(X_pca)

  ax[fig_index].imshow(X_r[1, :].reshape(28, 28), interpolation='none', cmap="gray")
  ax[fig_index].set_title("# componentes principales: {}".format(p), fontsize = 10)
  ax[fig_index].set_xticks([]) 
  ax[fig_index].set_yticks([]) 

  fig_index += 1

plt.show()

"""**Quinto paso:** Visualizar los datos en un espacio de dimensión reducida formado por la 1era vs. la 2da componentes principales, usando scatterplot."""

fig, ax = plt.subplots(figsize = (10, 6))

ax.scatter(X_pca[:,0], X_pca[:,1], alpha = 0.65)
ax.set_xlabel('Primer componente principal')
ax.set_ylabel('Segunda componente principal')
ax.grid()

"""**Sexto paso:** Hacer k-means con k=2 sobre los datos del espacio 2D formado por la primera y segunda componente principal. Visualizar algunos ejemplos correspondientes a cada uno de los clusters para entender a dónde fue a parar cada tipo de prenda en los clusters."""

from sklearn.cluster import KMeans

pca = PCA(n_components = 2)
pca.fit(X_train_scaled)
X_pca = pca.transform(X_train_scaled)

kmeans = KMeans(n_clusters=2)
kmeans.fit(X_pca)

print(kmeans.labels_)
plt.imshow(X_test[5998].reshape(28,28), interpolation='none', cmap="gray")
plt.show()
plt.imshow(X_test[5999].reshape(28,28), interpolation='none', cmap="gray")
plt.show()

"""**Séptimo paso:** Usando los ids del clustering como etiquetas, entrenar un modelo KNN (K=5) usando los datos de entrenamiento (con todos los features, no solo las primeras dos componentes principales)."""

from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from random import shuffle

clf = KNeighborsClassifier(n_neighbors=5)
clf.fit(X_train_scaled, y_train)

"""**Octavo paso:** Armar un nuevo vector de etiquetas que reemplace a y_test, y tenga 0 donde pensamos que ese ejemplo iría al cluster 0 y 1 donde pensamos que iría al cluster 1 (los clusters que obtuvimos con k-means)."""

y_test2=[]

for i in range(len(y_test)):
  if y_test[i] in [5,7,8,9]:
    y_test2 += [0]
  else:
    y_test2 += [1]

print(y_test2)

"""**Noveno paso:** Ahora aplicar el modelo KNN a datos del dataset de evaluación. Hacerlo con los primeros 1000 (sino puede tardar mucho). Usando las ids obtenidas mediante el procedimiento del paso anterior, y las predichas por KNN, construir una matriz de confusión para evaluar la performance del clasificador KNN."""

y_test2 = np.array(y_test2)
y_pred = clf.predict(X_test_scaled[:100]) #Aplico el modelo para los primeros 100 datos. Porque aún con 1000 tarda demasiado.
from sklearn.metrics import confusion_matrix as cm
conf_mat = cm(y_test2[:100], y_pred[:100])
print(conf_mat)